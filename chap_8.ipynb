{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_metric, load_dataset\n",
    "from transformers import pipeline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ckpt = 'transformersbook/bert-base-uncased-finetuned-clinic'\n",
    "pipe = pipeline('text-classification', model=ckpt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "\n",
    "    def __init__(self, pipeline, dataset, optim_type='BERT baseline'):\n",
    "        self.pipeline = pipeline\n",
    "        self.dataset = dataset\n",
    "        self.optim_type = optim_type\n",
    "        self.score = load_metric('accuracy')\n",
    "\n",
    "    def compute_accuracy(self, mapper):\n",
    "        preds, labels = [], []\n",
    "        for example in self.dataset:\n",
    "            pred = self.pipeline(example['text'])[0]['label']\n",
    "            label = example['intent']\n",
    "            if mapper:\n",
    "                preds.append(mapper(pred))\n",
    "            else:\n",
    "                preds.append(pred)\n",
    "            labels.append(label)\n",
    "        results = self.score.compute(predictions=preds, references=labels)\n",
    "        print(f'Accuracy on test set: {results[\"accuracy\"]:.3f}')\n",
    "        return results\n",
    "\n",
    "    def compute_size(self):\n",
    "        state = self.pipeline.model.state_dict()\n",
    "        fp = pathlib.Path('model.pt')\n",
    "        torch.save(state, fp)\n",
    "        mb = pathlib.Path(fp).stat().st_size / (1024**2)\n",
    "        fp.unlink(missing_ok=True)\n",
    "        print(f'Model size (MB): {mb:.2f}')\n",
    "        return {'size_mb': mb}\n",
    "\n",
    "    def time_pipeline(self, query='What is the pin number for my account?'):\n",
    "        latencies = []\n",
    "        for _ in range(100):\n",
    "            start = perf_counter()\n",
    "            _ = self.pipeline(query)\n",
    "            latencies.append(perf_counter() - start)\n",
    "        avg_ms = np.mean(latencies) * 1000\n",
    "        std_ms = np.std(latencies) * 1000\n",
    "        print(f'Average latency (ms): {avg_ms:.2f} +/- {std_ms:.2f}')\n",
    "        return {'avg_ms': avg_ms, 'std_ms': std_ms}\n",
    "\n",
    "    def run(self, query=None, mapper=None):\n",
    "        if query is None:\n",
    "            query='What is the pin number for my account?'\n",
    "        metrics = {\n",
    "            self.optim_type: {self.compute_size()}\n",
    "        }\n",
    "        metrics[self.optim_type].update(self.time_pipeline(query))\n",
    "        metrics[self.optim_type].update(self.compute_accuracy(mapper))\n",
    "        return metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = load_dataset('clinic_oos', 'plus')\n",
    "data['test'][42]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "intents = data['test'].features['intent']\n",
    "get_intents = lambda x: intents.int2str(x)  # mapper func"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pb = PerformanceBenchmark(pipe, data['test'])\n",
    "perf = pb.run(mapper=get_intents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}